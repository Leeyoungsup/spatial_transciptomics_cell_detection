import tqdm
import torch
import utils.util as util
import os
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


def plot_training_progress(train_losses, val_det_recalls, val_cls_accs, val_macro_precisions, 
                          val_macro_recalls, val_macro_f1s, epoch, save_dir, class_stats_history=None):
    """í•™ìŠµ ì§„í–‰ ìƒí™©ì„ 4x2 subplotìœ¼ë¡œ ì‹œê°í™”í•˜ê³  ì €ì¥í•˜ëŠ” í•¨ìˆ˜ (Point-label ë©”íŠ¸ë¦­ ì „ìš© + í´ë˜ìŠ¤ë³„ ì„±ëŠ¥)"""
    
    # í´ë˜ìŠ¤ë³„ í†µê³„ í¬í•¨ ì—¬ë¶€ì— ë”°ë¼ subplot êµ¬ì„± ë³€ê²½
    if class_stats_history is not None and len(class_stats_history) > 0:
        fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8)) = plt.subplots(4, 2, figsize=(15, 24))
    else:
        fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(15, 18))
    
    epochs_range = range(1, len(train_losses) + 1)
    
    # 1. Training Loss
    ax1.plot(epochs_range, train_losses, 'b-', linewidth=2, label='Train Loss')
    ax1.set_title('Training Loss', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    
    # 2. Macro F1-score (ì£¼ìš” ì§€í‘œ)
    ax2.plot(epochs_range, val_macro_f1s, 'darkgreen', linewidth=2, label='Macro F1-score â­')
    ax2.set_title('Macro F1-score (Primary Metric)', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Macro F1')
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    
    # 3. Detection Recall
    ax3.plot(epochs_range, val_det_recalls, 'cyan', linewidth=2, label='Detection Recall')
    ax3.set_title('Detection Recall (GT ì¤‘ ì°¾ì€ ë¹„ìœ¨)', fontsize=14, fontweight='bold')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Detection Recall')
    ax3.grid(True, alpha=0.3)
    ax3.legend()
    
    # 4. Classification Accuracy
    ax4.plot(epochs_range, val_cls_accs, 'magenta', linewidth=2, label='Classification Accuracy')
    ax4.set_title('Classification Accuracy (ë¶„ë¥˜ ì •í™•ë„)', fontsize=14, fontweight='bold')
    ax4.set_xlabel('Epoch')
    ax4.set_ylabel('Accuracy')
    ax4.grid(True, alpha=0.3)
    ax4.legend()
    
    # 5. Macro Precision & Recall
    ax5.plot(epochs_range, val_macro_precisions, 'orange', linewidth=2, label='Macro Precision')
    ax5.plot(epochs_range, val_macro_recalls, 'red', linewidth=2, label='Macro Recall')
    ax5.set_title('Macro Precision & Recall', fontsize=14, fontweight='bold')
    ax5.set_xlabel('Epoch')
    ax5.set_ylabel('Score')
    ax5.grid(True, alpha=0.3)
    ax5.legend()
    
    # 6. Detection & Classification í†µí•©
    ax6.plot(epochs_range, val_det_recalls, 'cyan', linewidth=2, label='Detection Recall', alpha=0.7)
    ax6.plot(epochs_range, val_cls_accs, 'magenta', linewidth=2, label='Classification Accuracy', alpha=0.7)
    ax6.plot(epochs_range, val_macro_f1s, 'darkgreen', linewidth=3, label='Macro F1 â­')
    ax6.set_title('Overall Performance', fontsize=14, fontweight='bold')
    ax6.set_xlabel('Epoch')
    ax6.set_ylabel('Score')
    ax6.grid(True, alpha=0.3)
    ax6.legend()
    
    # 7. í´ë˜ìŠ¤ë³„ F1-score (class_stats_historyê°€ ìˆì„ ê²½ìš°)
    if class_stats_history is not None and len(class_stats_history) > 0:
        class_names = ['class_0', 'class_1+', 'class_2+', 'class_3+']
        class_colors = ['green', 'gold', 'blue', 'red']
        
        # í´ë˜ìŠ¤ë³„ F1 ë°ì´í„° ì¶”ì¶œ
        for class_idx, (class_name, color) in enumerate(zip(class_names, class_colors)):
            class_f1_values = []
            for stats in class_stats_history:
                if class_name in stats:
                    class_f1_values.append(stats[class_name]['f1'])
                else:
                    class_f1_values.append(0)
            ax7.plot(epochs_range, class_f1_values, color=color, linewidth=2, 
                    label=class_name, marker='o', markersize=3, alpha=0.8)
        
        ax7.set_title('Per-Class F1-Score', fontsize=14, fontweight='bold')
        ax7.set_xlabel('Epoch')
        ax7.set_ylabel('F1-Score')
        ax7.grid(True, alpha=0.3)
        ax7.legend()
        ax7.set_ylim([0, 1])
        
        # 8. í´ë˜ìŠ¤ë³„ Precision & Recall
        for class_idx, (class_name, color) in enumerate(zip(class_names, class_colors)):
            class_precision_values = []
            class_recall_values = []
            for stats in class_stats_history:
                if class_name in stats:
                    class_precision_values.append(stats[class_name]['precision'])
                    class_recall_values.append(stats[class_name]['recall'])
                else:
                    class_precision_values.append(0)
                    class_recall_values.append(0)
            
            # Precision (ì‹¤ì„ )
            ax8.plot(epochs_range, class_precision_values, color=color, linewidth=2, 
                    linestyle='-', label=f'{class_name} P', alpha=0.7)
            # Recall (ì ì„ )
            ax8.plot(epochs_range, class_recall_values, color=color, linewidth=2, 
                    linestyle='--', label=f'{class_name} R', alpha=0.7)
        
        ax8.set_title('Per-Class Precision (â€”) & Recall (- -)', fontsize=14, fontweight='bold')
        ax8.set_xlabel('Epoch')
        ax8.set_ylabel('Score')
        ax8.grid(True, alpha=0.3)
        ax8.legend(ncol=2, fontsize=9)
        ax8.set_ylim([0, 1])
    
    # ì „ì²´ ì œëª©
    if class_stats_history is not None and len(class_stats_history) > 0:
        fig.suptitle(f'Training Progress (Point-Label Metrics + Per-Class Stats) - Epoch {epoch}', 
                     fontsize=16, fontweight='bold')
    else:
        fig.suptitle(f'Training Progress (Point-Label Metrics) - Epoch {epoch}', 
                     fontsize=16, fontweight='bold')
    
    # ìµœì‹  ê°’ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
    if len(train_losses) > 0:
        latest_info = f"""Latest Values (Epoch {len(train_losses)}):
Train Loss: {train_losses[-1]:.4f}
Macro F1: {val_macro_f1s[-1]:.4f} | Det Recall: {val_det_recalls[-1]:.4f} | Cls Acc: {val_cls_accs[-1]:.4f}
Macro Precision: {val_macro_precisions[-1]:.4f} | Macro Recall: {val_macro_recalls[-1]:.4f}
Best Macro F1: {max(val_macro_f1s):.4f} (Epoch {val_macro_f1s.index(max(val_macro_f1s))+1})"""
        
        # í´ë˜ìŠ¤ë³„ í†µê³„ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if class_stats_history is not None and len(class_stats_history) > 0:
            latest_class_stats = class_stats_history[-1]
            latest_info += "\n\nPer-Class F1 (Latest):"
            class_names = ['class_0', 'class_1+', 'class_2+', 'class_3+']
            for class_name in class_names:
                if class_name in latest_class_stats:
                    f1_val = latest_class_stats[class_name]['f1']
                    latest_info += f"\n  {class_name}: {f1_val:.4f}"
        
        fig.text(0.02, 0.02, latest_info, fontsize=10, 
                bbox=dict(boxstyle="round,pad=0.5", facecolor='lightblue', alpha=0.8))
    
    plt.tight_layout()
    plt.subplots_adjust(top=0.95, bottom=0.10)
    
    # ì €ì¥
    save_path = os.path.join(save_dir, f'training_progress_epoch_{epoch}.png')
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"ğŸ“ˆ í•™ìŠµ ì§„í–‰ ê·¸ë˜í”„ ì €ì¥: {save_path}")
    
    # plt.show()
    # plt.close()


def visualize_ground_truth_and_prediction_separately(model, dataset, idx=0, conf_threshold=0.5, iou_threshold=0.3, epoch=None, save_dir=None):
    """ì‹¤ì œ ë¼ë²¨ê³¼ ì˜ˆì¸¡ ë¼ë²¨ì„ subplotìœ¼ë¡œ ì¢Œìš°ì— í‘œì‹œí•˜ëŠ” í•¨ìˆ˜ (tissue context ì§€ì›)"""
    if len(dataset) <= idx:
        print(f"ê²½ê³ : ë°ì´í„°ì…‹ì´ ë¹„ì–´ ìˆê±°ë‚˜ idx {idx}ê°€ ë°ì´í„°ì…‹ í¬ê¸°({len(dataset)})ë³´ë‹¤ í½ë‹ˆë‹¤.")
        return
    
    model.eval()
    img, tissue_img, cls, box, _ = dataset[idx]
    
    # í•˜ë‚˜ì˜ figureì— 2ê°œì˜ subplot ìƒì„± (1í–‰ 2ì—´)
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
    img = img.cpu() / 255.
    # Subplot 1: Ground Truth (ì‹¤ì œ ë¼ë²¨)
    ax1.imshow(img.permute(1, 2, 0).cpu().numpy())
    class_names = {
    0: "epithelial",
    1: "Basal/Myoepithelial",
    2: "Smooth muscle",
    3: "Fibroblast",
    4: "Endothelial",
    5: "Lymphocyte",                # T + B í†µí•©
    6: "Plasma cell",
    7: "Macrophage/Histiocyte",     # í†µí•©
    8: "Neutrophil",
    9: "Adipocyte",
    10: "Other/Unknown"
}
    colors = ["#FF0000","#FFA500",
    "#8B4513",
    "#00FF00",
    "#0000FF",
    "#FFFF00",
    "#FF00FF",
    "#9400D3",
    "#00FFFF",
    "#FF6060",
    "#808080"]
    for i in range(len(cls)):
        class_id = int(cls[i].item())
        x_center, y_center, w, h = box[i].tolist()
        
        x = (x_center - w/2) * img.shape[2]
        y = (y_center - h/2) * img.shape[1]
        w_box = w * img.shape[2]
        h_box = h * img.shape[1]
        color=colors[class_id]
        # ì¤‘ì‹¬ì  í‘œì‹œ
        # ì¤‘ì‹¬ì  ì¢Œí‘œ ê³„ì‚°
        center_x = int(x + w_box / 2)
        center_y = int(y + h_box / 2)

        ax1.scatter(center_x, center_y, facecolors='none',  s=20, marker='o', edgecolors=color, linewidths=1)

    gt_title = f'Ground Truth (Tissue Context)'
    if epoch is not None:
        gt_title += f' - Epoch {epoch}'
    ax1.set_title(gt_title, fontsize=16, fontweight='bold')
    ax1.axis('off')
    
    # Subplot 2: Model Prediction (ì˜ˆì¸¡ ë¼ë²¨)
    ax2.imshow(img.permute(1, 2, 0).cpu().numpy())
    tissue_img= tissue_img.cpu() / 255.
    prediction_count = 0
    with torch.no_grad():
        img_input = img.unsqueeze(0).to(device)
        tissue_input = tissue_img.unsqueeze(0).to(device)
        with torch.amp.autocast('cuda'):
            pred = model(img_input, tissue_context=tissue_input)

        # NMS ì ìš©
        results = util.non_max_suppression(pred, confidence_threshold=conf_threshold, iou_threshold=iou_threshold)
        if len(results[0]) > 0:
            for *xyxy, conf, cls_id in results[0]:
                x1, y1, x2, y2 = xyxy
                x1, y1, x2, y2 = x1.item(), y1.item(), x2.item(), y2.item()
                w_pred = x2 - x1
                h_pred = y2 - y1
                
                
                color = colors[int(cls_id.item())]
                center_x = (x1 + x2)//2
                center_y = (y1 + y2)//2
                ax2.scatter(center_x, center_y, facecolors='none',  s=20, marker='o', edgecolors=color, linewidths=1)

                prediction_count += 1
        
        if prediction_count == 0:
            ax2.text(img.shape[2]//2, img.shape[1]//2, 'No Predictions', 
                     fontsize=20, color='white', ha='center', va='center',
                     bbox=dict(facecolor='red', alpha=0.8, pad=10))
    
    pred_title = f'Model Prediction - {prediction_count} detections'
    if epoch is not None:
        pred_title += f' - Epoch {epoch}'
    ax2.set_title(pred_title, fontsize=16, fontweight='bold')
    ax2.axis('off')
    
    # ì „ì²´ figure ì œëª© ì„¤ì •
    if epoch is not None:
        fig.suptitle(f'Validation Comparison - Epoch {epoch}, Sample {idx+1}', 
                     fontsize=18, fontweight='bold', y=0.95)
    
    # ë²”ë¡€ ì¶”ê°€ (12ê°œ í´ë˜ìŠ¤)
    legend_elements = [
        patches.Patch(color=colors[i], label=class_names[i]) for i in range(len(colors))
    ]
    fig.legend(handles=legend_elements, loc='lower center', ncol=6, 
               bbox_to_anchor=(0.5, 0.02), fontsize=12)
    
    # ë ˆì´ì•„ì›ƒ ì¡°ì •
    plt.tight_layout()
    plt.subplots_adjust(bottom=0.15, top=0.85)
    
    # ì €ì¥
    if save_dir and epoch:
        save_path = os.path.join(save_dir, f'validation_comparison_epoch_{epoch}.png')
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"âœ… ë¹„êµ ì´ë¯¸ì§€ ì €ì¥: {save_path}")
    
    # plt.show()
    plt.clf()
    
    
    
def compute_validation_metrics(model, val_loader, device, params):
    """ê²€ì¦ ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜ (mAP, precision, recall í¬í•¨) - loss ê³„ì‚° ì œê±°, ë¼ë²¨ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬"""
    model.eval()
    
    # Configure IoU thresholds for mAP calculation
    iou_v = torch.linspace(start=0.5, end=0.95, steps=10).to(device)  # iou vector for mAP@0.5:0.95
    n_iou = iou_v.numel()
    
    metrics = []
    
    with torch.no_grad():
        for val_images, val_targets in val_loader:
            val_images = val_images.to(device).float() / 255
            _, _, h, w = val_images.shape  # batch-size, channels, height, width
            scale = torch.tensor((w, h, w, h)).to(device)
            
            # ëª¨ë¸ ì˜ˆì¸¡ë§Œ ìˆ˜í–‰ (loss ê³„ì‚° ì œê±°)
            with torch.amp.autocast('cuda'):
                val_outputs = model(val_images)
            
            # NMS for metric calculation
            # Point ë¼ë²¨ì— ìµœì í™”ëœ threshold ì‚¬ìš©
            outputs = util.non_max_suppression(val_outputs, confidence_threshold=0.25, iou_threshold=0.45)
            
            # Metrics calculation
            for i, output in enumerate(outputs):
                idx = val_targets['idx'] == i
                cls = val_targets['cls'][idx]
                box = val_targets['box'][idx]
                
                # ë¼ë²¨ë„ ì—†ê³  ì˜ˆì¸¡ë„ ì—†ëŠ” ê²½ìš° - ì™„ì „íˆ ê±´ë„ˆë›°ê¸°
                if cls.shape[0] == 0 and output.shape[0] == 0:
                    continue
                
                # ë¼ë²¨ì€ ì—†ì§€ë§Œ ì˜ˆì¸¡ì´ ìˆëŠ” ê²½ìš° (False Positives)
                if cls.shape[0] == 0 and output.shape[0] > 0:
                    metric = torch.zeros(output.shape[0], n_iou, dtype=torch.bool).to(device)
                    metrics.append((metric, output[:, 4], output[:, 5], torch.tensor([], device=device)))
                    continue
                
                # ë¼ë²¨ì€ ìˆì§€ë§Œ ì˜ˆì¸¡ì´ ì—†ëŠ” ê²½ìš° (False Negatives)
                if cls.shape[0] > 0 and output.shape[0] == 0:
                    cls = cls.to(device)
                    metric = torch.zeros(0, n_iou, dtype=torch.bool).to(device)
                    metrics.append((metric, torch.zeros(0).to(device), torch.zeros(0).to(device), cls.squeeze(-1)))
                    continue
                
                # ë¼ë²¨ë„ ìˆê³  ì˜ˆì¸¡ë„ ìˆëŠ” ê²½ìš°ë§Œ ì •ìƒ ì²˜ë¦¬
                cls = cls.to(device)
                box = box.to(device)
                
                metric = torch.zeros(output.shape[0], n_iou, dtype=torch.bool).to(device)
                
                # Evaluate - clsì™€ boxê°€ ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
                try:
                    # cls ì°¨ì› í™•ì¸ ë° ì¡°ì •
                    if cls.dim() == 1:
                        cls_reshaped = cls.unsqueeze(1)  # [N] -> [N, 1]
                    else:
                        cls_reshaped = cls
                    
                    # boxë¥¼ xyxy í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    box_xyxy = util.wh2xy(box) * scale
                    
                    # target ìƒì„± [N, 5] (class, x1, y1, x2, y2)
                    target = torch.cat(tensors=(cls_reshaped, box_xyxy), dim=1)
                    metric = util.compute_metric(output[:, :6], target, iou_v)
                except Exception as e:
                    print(f"ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ (ê±´ë„ˆë›°ê¸°): {e}")
                    continue
                
                # Append
                metrics.append((metric, output[:, 4], output[:, 5], cls.squeeze(-1)))
    
    # Calculate mAP if we have metrics
    m_pre, m_rec, map50, mean_ap = 0, 0, 0, 0
    if len(metrics) > 0:
        try:
            # ê° ë©”íŠ¸ë¦­ ìš”ì†Œë¥¼ ì•ˆì „í•˜ê²Œ ê²°í•©
            stats = []
            for i in range(4):  # metric, conf, cls_pred, cls_true
                elements = []
                for metric_tuple in metrics:
                    if i < len(metric_tuple) and metric_tuple[i] is not None:
                        element = metric_tuple[i]
                        # í…ì„œë¥¼ numpyë¡œ ë³€í™˜í•˜ê³  ì°¨ì› í™•ì¸
                        if isinstance(element, torch.Tensor):
                            element_np = element.cpu().numpy()
                            # 0ì°¨ì› í…ì„œë¥¼ 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜
                            if element_np.ndim == 0:
                                element_np = np.array([element_np])
                            elements.append(element_np)
                        else:
                            elements.append(element)
                
                # ìš”ì†Œë“¤ì´ ìˆì„ ë•Œë§Œ concatenate
                if elements:
                    # ëª¨ë“  ìš”ì†Œê°€ ê°™ì€ ì°¨ì›ì¸ì§€ í™•ì¸
                    if all(isinstance(elem, np.ndarray) for elem in elements):
                        try:
                            concatenated = np.concatenate(elements, axis=0)
                            stats.append(concatenated)
                        except ValueError as ve:
                            print(f"Concatenation ì˜¤ë¥˜ (ì¸ë±ìŠ¤ {i}): {ve}")
                            stats.append(np.array([]))
                    else:
                        stats.append(np.array([]))
                else:
                    stats.append(np.array([]))
            
            # statsê°€ ì˜¬ë°”ë¥´ê²Œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if len(stats) == 4 and all(isinstance(s, np.ndarray) for s in stats):
                tp, fp, m_pre, m_rec, map50, mean_ap = util.compute_ap(*stats, plot=False, names=params["names"])
            else:
                print("ë©”íŠ¸ë¦­ í†µê³„ ìƒì„± ì‹¤íŒ¨")
                m_pre, m_rec, map50, mean_ap = 0, 0, 0, 0
                
        except Exception as e:
            print(f"mAP ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"ë©”íŠ¸ë¦­ ê°œìˆ˜: {len(metrics)}")
            if len(metrics) > 0:
                print(f"ì²« ë²ˆì§¸ ë©”íŠ¸ë¦­ êµ¬ì¡°: {[type(x) for x in metrics[0]]}")
                print(f"ì²« ë²ˆì§¸ ë©”íŠ¸ë¦­ í¬ê¸°: {[x.shape if hasattr(x, 'shape') else len(x) if hasattr(x, '__len__') else 'scalar' for x in metrics[0]]}")
            m_pre, m_rec, map50, mean_ap = 0, 0, 0, 0
    
    return m_pre, m_rec, map50, mean_ap


def compute_validation_metrics_with_kappa(model, val_loader, device, params):
    """Cohen's Kappaë¥¼ í¬í•¨í•œ ê²€ì¦ ë©”íŠ¸ë¦­ ê³„ì‚° - ê°œì„  ë²„ì „ (ë°°ê²½ ì—†ìŒ, 4ê°œ ì„¸í¬ í´ë˜ìŠ¤ë§Œ)"""
    try:
        from sklearn.metrics import cohen_kappa_score
        from scipy.optimize import linear_sum_assignment
    except ImportError:
        print("ê²½ê³ : scikit-learn ë˜ëŠ” scipyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ Cohen's Kappaë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        precision, recall, map50, mean_ap = compute_validation_metrics(model, val_loader, device, params)
        return precision, recall, map50, mean_ap, 0.0
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°
    precision, recall, map50, mean_ap = compute_validation_metrics(model, val_loader, device, params)
    
    # Cohen's Kappa ê³„ì‚° - ê°ì²´ ë§¤ì¹­ ê¸°ë°˜
    # ë°°ê²½ì´ ì—†ìœ¼ë¯€ë¡œ ë§¤ì¹­ëœ ê°ì²´ë§Œ ì‚¬ìš©
    model.eval()
    all_gt_classes = []
    all_pred_classes = []
    
    with torch.no_grad():
        for batch_idx, (images, targets) in enumerate(val_loader):
            images = images.to(device).float() / 255
            
            # ì˜ˆì¸¡
            with torch.amp.autocast('cuda'):
                pred = model(images)
            
            # NMS ì ìš©
            results = util.non_max_suppression(pred, confidence_threshold=0.25, iou_threshold=0.45)
            
            # ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ì²˜ë¦¬
            for i in range(len(images)):
                # Ground truth
                cls_targets = targets['cls']
                box_targets = targets['box']
                idx_targets = targets['idx']
                
                batch_mask = idx_targets == i
                if not batch_mask.any():
                    continue
                
                batch_cls = cls_targets[batch_mask].cpu().numpy()
                batch_box = box_targets[batch_mask].cpu().numpy()
                
                # Predictions
                if len(results) > i and len(results[i]) > 0:
                    pred_boxes = results[i][:, :4].cpu().numpy()  # xyxy
                    pred_classes = results[i][:, 5].cpu().numpy()  # class
                    
                    # GT boxë¥¼ xyxyë¡œ ë³€í™˜
                    gt_boxes_xyxy = []
                    for box in batch_box:
                        x_center, y_center, w, h = box
                        x1 = (x_center - w/2) * 512
                        y1 = (y_center - h/2) * 512
                        x2 = (x_center + w/2) * 512
                        y2 = (y_center + h/2) * 512
                        gt_boxes_xyxy.append([x1, y1, x2, y2])
                    gt_boxes_xyxy = np.array(gt_boxes_xyxy)
                    
                    # IoU í–‰ë ¬ ê³„ì‚°
                    iou_matrix = compute_iou_matrix(gt_boxes_xyxy, pred_boxes)
                    
                    # Hungarian Algorithmìœ¼ë¡œ ìµœì  ë§¤ì¹­
                    if iou_matrix.size > 0 and iou_matrix.shape[0] > 0 and iou_matrix.shape[1] > 0:
                        gt_indices, pred_indices = linear_sum_assignment(-iou_matrix)
                        
                        # IoU ì„ê³„ê°’ ì´ìƒì¸ ë§¤ì¹­ë§Œ ì‚¬ìš©
                        iou_threshold = 0.3
                        for gt_idx, pred_idx in zip(gt_indices, pred_indices):
                            if iou_matrix[gt_idx, pred_idx] >= iou_threshold:
                                all_gt_classes.append(int(batch_cls[gt_idx]))
                                all_pred_classes.append(int(pred_classes[pred_idx]))
                        
                        # ğŸ’¡ ë°°ê²½ì´ ì—†ìœ¼ë¯€ë¡œ ë§¤ì¹­ë˜ì§€ ì•Šì€ GTì™€ PredëŠ” Kappa ê³„ì‚°ì—ì„œ ì œì™¸
                        # False Negative/PositiveëŠ” Precision/Recallì—ì„œ ì²˜ë¦¬ë¨
    
    # Cohen's Kappa ê³„ì‚° (ë§¤ì¹­ëœ ê°ì²´ë§Œ ì‚¬ìš©)
    try:
        if len(all_gt_classes) > 0 and len(all_pred_classes) > 0:
            kappa = cohen_kappa_score(all_gt_classes, all_pred_classes)
        else:
            kappa = 0.0
            print("ê²½ê³ : ë§¤ì¹­ëœ ê°ì²´ê°€ ì—†ì–´ Kappaë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"Cohen's Kappa ê³„ì‚° ì˜¤ë¥˜: {e}")
        kappa = 0.0
    
    return precision, recall, map50, mean_ap, kappa


def compute_iou_matrix(boxes1, boxes2):
    """
    ë‘ ë°•ìŠ¤ ì§‘í•© ê°„ì˜ IoU í–‰ë ¬ ê³„ì‚°
    boxes: [N, 4] (x1, y1, x2, y2)
    """
    if len(boxes1) == 0 or len(boxes2) == 0:
        return np.zeros((len(boxes1), len(boxes2)))
    
    # ë©´ì  ê³„ì‚°
    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])
    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])
    
    # IoU í–‰ë ¬
    iou_matrix = np.zeros((len(boxes1), len(boxes2)))
    
    for i in range(len(boxes1)):
        for j in range(len(boxes2)):
            # êµì§‘í•©
            x1 = max(boxes1[i, 0], boxes2[j, 0])
            y1 = max(boxes1[i, 1], boxes2[j, 1])
            x2 = min(boxes1[i, 2], boxes2[j, 2])
            y2 = min(boxes1[i, 3], boxes2[j, 3])
            
            if x2 > x1 and y2 > y1:
                intersection = (x2 - x1) * (y2 - y1)
                union = area1[i] + area2[j] - intersection
                iou_matrix[i, j] = intersection / union if union > 0 else 0
    
    return iou_matrix


def get_kappa_interpretation(kappa):
    """Kappa ê°’ í•´ì„"""
    if kappa < 0: 
        return "Poor"
    elif kappa < 0.21: 
        return "Slight"
    elif kappa < 0.41: 
        return "Fair"  
    elif kappa < 0.61: 
        return "Moderate"
    elif kappa < 0.81: 
        return "Substantial"
    else: 
        return "Almost Perfect"


def quick_kappa_test(model, val_loader, device):
    """í˜„ì¬ ëª¨ë¸ì˜ Cohen's Kappa ë¹ ë¥¸ ì¸¡ì •"""
    try:
        from sklearn.metrics import cohen_kappa_score
    except ImportError:
        print("ê²½ê³ : scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ Cohen's Kappaë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 0.0
        
    model.eval()
    
    # ëª‡ ê°œ ìƒ˜í”Œë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
    sample_gt = []
    sample_pred = []
    
    with torch.no_grad():
        for i, (images, targets) in enumerate(val_loader):
            if i >= 10:  # 10ê°œ ë°°ì¹˜ë§Œ í…ŒìŠ¤íŠ¸
                break
                
            images = images.to(device).float() / 255
            pred = model(images)
            results = util.non_max_suppression(pred, confidence_threshold=0.25, iou_threshold=0.45)
            
            # ê°„ë‹¨í•œ ë¹„êµë¥¼ ìœ„í•´ ê°ì²´ ê°œìˆ˜ ê¸°ë°˜ ë¼ë²¨ë§
            gt_count = len(targets['cls'])
            pred_count = len(results[0]) if len(results) > 0 and len(results[0]) > 0 else 0
            
            # ë‹¨ìˆœí™”ëœ ë¼ë²¨ (0: ì—†ìŒ, 1: ì ìŒ, 2: ë§ìŒ)
            gt_label = 0 if gt_count == 0 else (1 if gt_count <= 5 else 2)
            pred_label = 0 if pred_count == 0 else (1 if pred_count <= 5 else 2)
            
            sample_gt.append(gt_label)
            sample_pred.append(pred_label)
    
    try:
        if len(sample_gt) > 0 and len(sample_pred) > 0:
            quick_kappa = cohen_kappa_score(sample_gt, sample_pred)
        else:
            quick_kappa = 0.0
    except Exception as e:
        print(f"ë¹ ë¥¸ Kappa ê³„ì‚° ì˜¤ë¥˜: {e}")
        quick_kappa = 0.0
    
    print(f"ğŸ“Š ë¹ ë¥¸ Cohen's Kappa ì¸¡ì •: {quick_kappa:.4f} ({get_kappa_interpretation(quick_kappa)})")
    return quick_kappa


def compute_distance_matrix(centers1, centers2):
    """
    ë‘ ì¤‘ì‹¬ì  ì§‘í•© ê°„ì˜ Euclidean ê±°ë¦¬ í–‰ë ¬ ê³„ì‚°
    centers: [N, 2] (x, y)
    """
    if len(centers1) == 0 or len(centers2) == 0:
        return np.zeros((len(centers1), len(centers2)))
    
    # ê±°ë¦¬ í–‰ë ¬ ê³„ì‚° (ë¸Œë¡œë“œìºìŠ¤íŒ… ì‚¬ìš©)
    centers1 = np.array(centers1).reshape(-1, 2)
    centers2 = np.array(centers2).reshape(-1, 2)
    
    # Euclidean ê±°ë¦¬: sqrt((x1-x2)^2 + (y1-y2)^2)
    diff = centers1[:, np.newaxis, :] - centers2[np.newaxis, :, :]  # [N1, N2, 2]
    distances = np.sqrt(np.sum(diff**2, axis=2))  # [N1, N2]
    
    return distances


def compute_point_label_metrics(model, val_loader, device, params, distance_threshold=16):
    """
    Point-labelì— ìµœì í™”ëœ ê²€ì¦ ë©”íŠ¸ë¦­ ê³„ì‚° (tissue context ì§€ì›)
    - Distance-based matching (IoU ëŒ€ì‹  ì¤‘ì‹¬ì  ê±°ë¦¬ ì‚¬ìš©)
    - Detection recall: GT ì„¸í¬ë¥¼ ì–¼ë§ˆë‚˜ ì°¾ì•˜ëŠ”ê°€
    - Classification accuracy: ì°¾ì€ ì„¸í¬ì˜ í´ë˜ìŠ¤ë¥¼ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ë¶„ë¥˜í–ˆëŠ”ê°€
    
    Args:
        model: YOLO ëª¨ë¸ (tissue context ì§€ì›)
        val_loader: ê²€ì¦ ë°ì´í„°ë¡œë”
        device: ë””ë°”ì´ìŠ¤
        params: íŒŒë¼ë¯¸í„° (í´ë˜ìŠ¤ ì´ë¦„ ë“±)
        distance_threshold: ë§¤ì¹­ ê±°ë¦¬ ì„ê³„ê°’ (í”½ì…€ ë‹¨ìœ„, ê¸°ë³¸ 16px)
    
    Returns:
        dict: {
            'detection_recall': GT ì¤‘ ë§¤ì¹­ëœ ë¹„ìœ¨,
            'classification_accuracy': ë§¤ì¹­ëœ ê°ì²´ ì¤‘ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜ëœ ë¹„ìœ¨,
            'macro_precision': í´ë˜ìŠ¤ë³„ í‰ê·  ì •ë°€ë„,
            'macro_recall': í´ë˜ìŠ¤ë³„ í‰ê·  ì¬í˜„ìœ¨,
            'macro_f1': í´ë˜ìŠ¤ë³„ í‰ê·  F1,
            'overall_recall': ì „ì²´ ì¬í˜„ìœ¨,
            'class_stats': í´ë˜ìŠ¤ë³„ ìƒì„¸ í†µê³„
        }
    """
    try:
        from scipy.optimize import linear_sum_assignment
    except ImportError:
        print("ê²½ê³ : scipyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ Point-label ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    model.eval()
    
    # ì „ì²´ í†µê³„
    total_gt = 0
    total_matched = 0
    total_correct_class = 0
    
    # í´ë˜ìŠ¤ë³„ í†µê³„ (12ê°œ í´ë˜ìŠ¤)
    num_classes = 12
    class_tp = np.zeros(num_classes)  # True Positive (ì˜¬ë°”ë¥´ê²Œ íƒì§€+ë¶„ë¥˜)
    class_fp = np.zeros(num_classes)  # False Positive (ì˜ëª» íƒì§€ ë˜ëŠ” ì˜ëª» ë¶„ë¥˜)
    class_fn = np.zeros(num_classes)  # False Negative (íƒì§€ ì‹¤íŒ¨)
    class_gt_count = np.zeros(num_classes)  # GT ê°œìˆ˜
    
    with torch.no_grad():
        for batch_idx, (images, tissue_images, targets) in enumerate(val_loader):
            images = images.to(device).float() / 255
            tissue_images = tissue_images.to(device).float() / 255
            
            # ì˜ˆì¸¡ (tissue context í¬í•¨)
            with torch.amp.autocast('cuda'):
                pred = model(images, tissue_context=tissue_images)
            
            # NMS ì ìš©
            results = util.non_max_suppression(pred, confidence_threshold=0.25, iou_threshold=0.45)
            
            # ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ì²˜ë¦¬
            for i in range(len(images)):
                # Ground truth ì¶”ì¶œ
                cls_targets = targets['cls']
                box_targets = targets['box']
                idx_targets = targets['idx']
                
                batch_mask = idx_targets == i
                if not batch_mask.any():
                    continue
                
                batch_cls = cls_targets[batch_mask].cpu().numpy()
                batch_box = box_targets[batch_mask].cpu().numpy()
                
                # GT ì¤‘ì‹¬ì  ê³„ì‚° (normalized -> pixel)
                img_size = 512  # ì´ë¯¸ì§€ í¬ê¸°
                gt_centers = []
                for box in batch_box:
                    x_center = box[0] * img_size
                    y_center = box[1] * img_size
                    gt_centers.append([x_center, y_center])
                gt_centers = np.array(gt_centers)
                
                # GT í´ë˜ìŠ¤ë³„ ì¹´ìš´íŠ¸
                for cls_id in batch_cls:
                    class_gt_count[int(cls_id)] += 1
                
                total_gt += len(batch_cls)
                
                # Predictions ì²˜ë¦¬
                if len(results) > i and len(results[i]) > 0:
                    pred_boxes = results[i][:, :4].cpu().numpy()  # xyxy
                    pred_classes = results[i][:, 5].cpu().numpy()  # class
                    
                    # Prediction ì¤‘ì‹¬ì  ê³„ì‚°
                    pred_centers = []
                    for box in pred_boxes:
                        x_center = (box[0] + box[2]) / 2
                        y_center = (box[1] + box[3]) / 2
                        pred_centers.append([x_center, y_center])
                    pred_centers = np.array(pred_centers)
                    
                    # ê±°ë¦¬ í–‰ë ¬ ê³„ì‚°
                    distance_matrix = compute_distance_matrix(gt_centers, pred_centers)
                    
                    # Hungarian Algorithmìœ¼ë¡œ ìµœì  ë§¤ì¹­
                    if distance_matrix.size > 0 and distance_matrix.shape[0] > 0 and distance_matrix.shape[1] > 0:
                        gt_indices, pred_indices = linear_sum_assignment(distance_matrix)
                        
                        # ê±°ë¦¬ ì„ê³„ê°’ ì´í•˜ì¸ ë§¤ì¹­ë§Œ ì‚¬ìš©
                        matched_gt = set()
                        matched_pred = set()
                        
                        for gt_idx, pred_idx in zip(gt_indices, pred_indices):
                            if distance_matrix[gt_idx, pred_idx] <= distance_threshold:
                                matched_gt.add(gt_idx)
                                matched_pred.add(pred_idx)
                                total_matched += 1
                                
                                gt_cls = int(batch_cls[gt_idx])
                                pred_cls = int(pred_classes[pred_idx])
                                
                                # í´ë˜ìŠ¤ê°€ ì¼ì¹˜í•˜ë©´ TP
                                if gt_cls == pred_cls:
                                    total_correct_class += 1
                                    class_tp[gt_cls] += 1
                                else:
                                    # í´ë˜ìŠ¤ ë¶ˆì¼ì¹˜: GTëŠ” FN, PredëŠ” FP
                                    class_fn[gt_cls] += 1
                                    class_fp[pred_cls] += 1
                        
                        # ë§¤ì¹­ë˜ì§€ ì•Šì€ GT: False Negative
                        for gt_idx in range(len(batch_cls)):
                            if gt_idx not in matched_gt:
                                gt_cls = int(batch_cls[gt_idx])
                                class_fn[gt_cls] += 1
                        
                        # ë§¤ì¹­ë˜ì§€ ì•Šì€ Pred: False Positive
                        for pred_idx in range(len(pred_classes)):
                            if pred_idx not in matched_pred:
                                pred_cls = int(pred_classes[pred_idx])
                                class_fp[pred_cls] += 1
                    else:
                        # ë§¤ì¹­ ë¶ˆê°€ëŠ¥: ëª¨ë“  GTëŠ” FN
                        for cls_id in batch_cls:
                            class_fn[int(cls_id)] += 1
                else:
                    # ì˜ˆì¸¡ ì—†ìŒ: ëª¨ë“  GTëŠ” FN
                    for cls_id in batch_cls:
                        class_fn[int(cls_id)] += 1
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    detection_recall = total_matched / total_gt if total_gt > 0 else 0
    classification_accuracy = total_correct_class / total_matched if total_matched > 0 else 0
    
    # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­
    class_precision = []
    class_recall = []
    class_f1 = []
    
    for c in range(num_classes):
        # Precision = TP / (TP + FP)
        precision = class_tp[c] / (class_tp[c] + class_fp[c]) if (class_tp[c] + class_fp[c]) > 0 else 0
        
        # Recall = TP / (TP + FN)
        recall = class_tp[c] / (class_tp[c] + class_fn[c]) if (class_tp[c] + class_fn[c]) > 0 else 0
        
        # F1 = 2 * (Precision * Recall) / (Precision + Recall)
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        class_precision.append(precision)
        class_recall.append(recall)
        class_f1.append(f1)
    
    # Macro-averaged ë©”íŠ¸ë¦­ (í´ë˜ìŠ¤ë³„ í‰ê· )
    macro_precision = np.mean(class_precision)
    macro_recall = np.mean(class_recall)
    macro_f1 = np.mean(class_f1)
    
    # Overall Recall (ì „ì²´ ì¬í˜„ìœ¨)
    overall_recall = np.sum(class_tp) / np.sum(class_tp + class_fn) if np.sum(class_tp + class_fn) > 0 else 0
    
    # í´ë˜ìŠ¤ë³„ ìƒì„¸ í†µê³„
    class_names = params.get('names', {})
    class_stats = {}
    for c in range(num_classes):
        class_name = class_names.get(c, f'Class_{c}')
        class_stats[class_name] = {
            'precision': class_precision[c],
            'recall': class_recall[c],
            'f1': class_f1[c],
            'tp': int(class_tp[c]),
            'fp': int(class_fp[c]),
            'fn': int(class_fn[c]),
            'gt_count': int(class_gt_count[c])
        }
    
    return {
        'detection_recall': detection_recall,
        'classification_accuracy': classification_accuracy,
        'macro_precision': macro_precision,
        'macro_recall': macro_recall,
        'macro_f1': macro_f1,
        'overall_recall': overall_recall,
        'class_stats': class_stats
    }