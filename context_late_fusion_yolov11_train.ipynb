{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f827039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import warnings\n",
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from torch.utils import data\n",
    "# ê°œë³„ json ë¼ë²¨ íŒŒì¼ì„ ì´ìš©í•´ í•™ìŠµ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from nets import ContextNn\n",
    "from utils import util\n",
    "import pandas as pd\n",
    "from utils.dataset import Dataset\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.model_selection import train_test_split\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\",device)\n",
    "with open('utils/args.yaml', errors='ignore') as f:\n",
    "    params = yaml.safe_load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e71dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_and_label(crop_h,crop_w,label,input_size):\n",
    "    temp_labels = []\n",
    "    for k in range(len(label)):\n",
    "        x = label[k][2]\n",
    "        y = label[k][1]\n",
    "        w = label[k][4]\n",
    "        h = label[k][3]\n",
    "        if x >= crop_h and y >= crop_w and x <= crop_h + input_size and y <= crop_w + input_size:\n",
    "            label[k][1] = (y+h/2 - crop_w)/ input_size\n",
    "            label[k][2] = (x+w/2 - crop_h)/ input_size\n",
    "            label[k][3] = (h) / input_size\n",
    "            label[k][4] = (w) / input_size\n",
    "            temp_labels.append(label[k])\n",
    "    return temp_labels\n",
    "    \n",
    "def find_key_by_value(dictionary, target_value):\n",
    "    \"\"\"valueë¡œ key ì°¾ê¸°\"\"\"\n",
    "    for key, value in dictionary.items():\n",
    "        if value == target_value:\n",
    "            return key\n",
    "    return None \n",
    "\n",
    "input_size = 512\n",
    "label_dir = '../../data/Brest_spatialTranscriptome/preprocessed_xenium/patch_train_data/**/annotation/'\n",
    "label_files = sorted(glob.glob(f\"{label_dir}/*.csv\"))\n",
    "image_filenames = []\n",
    "tissue_image_filenames = []\n",
    "labels = []\n",
    "# ë¼ë²¨ì´ ì¡´ì¬í•˜ëŠ” ì´ë¯¸ì§€\n",
    "for label_file in label_files:\n",
    "    data1 = pd.read_csv(label_file)\n",
    "\n",
    "    img_path = label_file.replace('/annotation', '/image').replace('.csv', '.png')\n",
    "    tissue_image_path=label_file.replace('/annotation', '/tissue_image').replace('.csv', '.png')\n",
    "    if os.path.exists(img_path):\n",
    "        image_filenames.append(img_path)\n",
    "        tissue_image_filenames.append(tissue_image_path)\n",
    "        arr = data1.to_numpy()\n",
    "        arr=arr[:, [4, 1, 0 , 3, 2]]#class, y, x, h, w\n",
    "        temp_labels=list(arr)\n",
    "        labels.append(temp_labels)\n",
    "\n",
    "images = []\n",
    "tissue_image=[]\n",
    "for i in tqdm(range(len(image_filenames))):\n",
    "    image =cv2.imread(image_filenames[i])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # BGR -> RGB ë³€í™˜\n",
    "    tissue_img = cv2.imread(tissue_image_filenames[i])\n",
    "    tissue_img = cv2.cvtColor(tissue_img, cv2.COLOR_BGR2RGB)  # BGR -> RGB ë³€í™˜\n",
    "    tissue_image.append(tissue_img)\n",
    "    images.append(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c0666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn1(batch):\n",
    "    \"\"\"Tissue contextë¥¼ í¬í•¨í•œ ë°°ì¹˜ collation\"\"\"\n",
    "    samples, context_samples, cls, box, indices = zip(*batch)\n",
    "\n",
    "    cls = torch.cat(cls, dim=0)\n",
    "    box = torch.cat(box, dim=0)\n",
    "\n",
    "    new_indices = list(indices)\n",
    "    for i in range(len(indices)):\n",
    "        new_indices[i] += i\n",
    "    indices = torch.cat(new_indices, dim=0)\n",
    "\n",
    "    targets = {'cls': cls,\n",
    "                'box': box,\n",
    "                'idx': indices}\n",
    "    return torch.stack(samples, dim=0), torch.stack(context_samples, dim=0), targets\n",
    "\n",
    "class custom_dataset(data.Dataset):\n",
    "    def __init__(self, images, params, augment, labels, image_infos=None):\n",
    "        self.params = params\n",
    "        self.mosaic = augment\n",
    "        self.augment = augment\n",
    "        self.images = images[0]\n",
    "        self.tissue_images = images[1]\n",
    "        self.labels = labels\n",
    "        self.scale_list = [256, 288, 320, 352, 384, 416, 448, 480, 512]\n",
    "        self.current_scale = 512  # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ê´€ë¦¬ë  í˜„ì¬ ìŠ¤ì¼€ì¼\n",
    "        self.n = len(self.images)\n",
    "        self.indices = list(range(self.n))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def set_scale(self, scale):\n",
    "        \"\"\"ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìŠ¤ì¼€ì¼ ì„¤ì •\"\"\"\n",
    "        self.current_scale = scale\n",
    "    \n",
    "    def random_multi_scale(self, image, label):\n",
    "        # í˜„ì¬ ì„¤ì •ëœ ìŠ¤ì¼€ì¼ ì‚¬ìš© (ë°°ì¹˜ ë‚´ ëª¨ë“  ì´ë¯¸ì§€ê°€ ë™ì¼í•œ í¬ê¸°)\n",
    "        size = self.current_scale\n",
    "        image = cv2.resize(image, (size, size))\n",
    "        temp_labels = []\n",
    "        for i in range(len(label)):\n",
    "            x_center = label[i][1]\n",
    "            y_center = label[i][2]\n",
    "            w = label[i][3]\n",
    "            h = label[i][4]\n",
    "            temp_labels.append([label[i][0], y_center, x_center, h, w])\n",
    "        return image, temp_labels\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        index = self.indices[index]\n",
    "        image = self.images[index].copy()\n",
    "        tissue_image = self.tissue_images[index].copy()\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        image,label=self.random_multi_scale(image,label)\n",
    "        cls = []\n",
    "        box = []\n",
    "        for i in range(len(label)):\n",
    "            cls.append(label[i][0])\n",
    "            box.append(label[i][1:5])\n",
    "        cls = np.array(cls)\n",
    "        box = np.array(box)\n",
    "        nl = len(box)\n",
    "        \n",
    "        if self.augment:\n",
    "            # Flip up-down (ë©”ì¸ ì´ë¯¸ì§€ì™€ tissue context ë™ì‹œ ì ìš©)\n",
    "            if random.random() < self.params['flip_ud']:\n",
    "                image = np.flipud(image).copy()\n",
    "                tissue_image = np.flipud(tissue_image).copy()\n",
    "                if nl:\n",
    "                    box[:, 1] = 1 - box[:, 1]\n",
    "            \n",
    "            # Flip left-right (ë©”ì¸ ì´ë¯¸ì§€ì™€ tissue context ë™ì‹œ ì ìš©)\n",
    "            if random.random() < self.params['flip_lr']:\n",
    "                image = np.fliplr(image).copy()\n",
    "                tissue_image = np.fliplr(tissue_image).copy()\n",
    "                if nl:\n",
    "                    box[:, 0] = 1 - box[:, 0]\n",
    "\n",
    "        # HWC -> CHW ë³€í™˜\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        tissue_image = tissue_image.transpose((2, 0, 1))\n",
    "        \n",
    "        return (torch.from_numpy(image), \n",
    "                torch.from_numpy(tissue_image),  # tissue context ì¶”ê°€\n",
    "                torch.from_numpy(cls), \n",
    "                torch.from_numpy(box), \n",
    "                torch.zeros(nl))\n",
    "    \n",
    "\n",
    "split=[0.9, 0.1]\n",
    "patch_train, patch_test, tissue_train, tissue_test, label_train, label_test = train_test_split(\n",
    "    images, tissue_image, labels, \n",
    "    test_size=0.15,\n",
    "    random_state=242,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Dataset ìƒì„±\n",
    "train_dataset = custom_dataset((patch_train, tissue_train), params, augment=True, labels=label_train)\n",
    "val_dataset = custom_dataset((patch_test, tissue_test), params, augment=False, labels=label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a4340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample_with_overlay(dataset, index=0):\n",
    "    \"\"\"\n",
    "    ë°ì´í„°ì…‹ì˜ íŠ¹ì • ìƒ˜í”Œì— ëŒ€í•´ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì˜¤ë²„ë ˆì´í•œ ì´ë¯¸ì§€ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.\n",
    "    ë©”ì¸ ì´ë¯¸ì§€ì™€ tissue context ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ í‘œì‹œí•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        dataset: ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹\n",
    "        index: ì‹œê°í™”í•  ìƒ˜í”Œì˜ ì¸ë±ìŠ¤\n",
    "    \"\"\"\n",
    "    # ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸° (tissue context í¬í•¨)\n",
    "    image_tensor, tissue_tensor, cls_tensor, box_tensor, _ = dataset[index]\n",
    "    \n",
    "    # í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜\n",
    "    # ì´ë¯¸ì§€: (C, H, W) -> (H, W, C)\n",
    "    image = image_tensor.numpy().transpose(1, 2, 0)\n",
    "    tissue_image = tissue_tensor.numpy().transpose(1, 2, 0)\n",
    "    cls = cls_tensor.numpy()\n",
    "    boxes = box_tensor.numpy()\n",
    "\n",
    "    # ì´ë¯¸ì§€ í¬ê¸°\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # í´ë˜ìŠ¤ ì´ë¦„ ë° ìƒ‰ìƒ ì„¤ì •\n",
    "    class_names = {\n",
    "    0: \"epithelial\",\n",
    "    1: \"Basal/Myoepithelial\",\n",
    "    2: \"Smooth muscle\",\n",
    "    3: \"Fibroblast\",\n",
    "    4: \"Endothelial\",\n",
    "    5: \"Lymphocyte\",                # T + B í†µí•©\n",
    "    6: \"Plasma cell\",\n",
    "    7: \"Macrophage/Histiocyte\",     # í†µí•©\n",
    "    8: \"Neutrophil\",\n",
    "    9: \"Adipocyte\",\n",
    "    10: \"Other/Unknown\"\n",
    "}\n",
    "    colors = [\"#FF0000\",\"#FFA500\",\n",
    "    \"#8B4513\",\n",
    "    \"#00FF00\",\n",
    "    \"#0000FF\",\n",
    "    \"#FFFF00\",\n",
    "    \"#FF00FF\",\n",
    "    \"#9400D3\",\n",
    "    \"#00FFFF\",\n",
    "    \"#FF6060\",\n",
    "    \"#808080\"]\n",
    "\n",
    "    \n",
    "    # ì‹œê°í™”: ë©”ì¸ ì´ë¯¸ì§€ì™€ tissue contextë¥¼ ë‚˜ë€íˆ í‘œì‹œ\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # ë©”ì¸ ì´ë¯¸ì§€\n",
    "    axes[0].imshow(image.astype(np.uint8))\n",
    "    for i in range(len(boxes)):\n",
    "        center_x, center_y, box_width, box_height = boxes[i]\n",
    "        center_x_pixel = center_x * width\n",
    "        center_y_pixel = center_y * height\n",
    "        box_w_pixel = box_width * width\n",
    "        box_h_pixel = box_height * height\n",
    "        \n",
    "        # ì¢Œìƒë‹¨ ì¢Œí‘œ ê³„ì‚° (ì¤‘ì‹¬ì  -> ì¢Œìƒë‹¨)\n",
    "        x1 = center_x_pixel - box_w_pixel / 2\n",
    "        y1 = center_y_pixel - box_h_pixel / 2\n",
    "        \n",
    "        class_id = int(cls[i])\n",
    "        color = colors[class_id] if class_id < len(colors) else 'green'\n",
    "        \n",
    "        # Bounding box ê·¸ë¦¬ê¸°\n",
    "        rect = patches.Rectangle((x1, y1), box_w_pixel, box_h_pixel, \n",
    "                                 linewidth=2, edgecolor=color, facecolor='none')\n",
    "        axes[0].add_patch(rect)\n",
    "    \n",
    "    axes[0].set_title(f'Main Image - Sample {index}', fontsize=12, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Tissue Context ì´ë¯¸ì§€\n",
    "    axes[1].imshow(tissue_image.astype(np.uint8))\n",
    "    axes[1].set_title(f'Tissue Context - Sample {index}', fontsize=12, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    fig.suptitle(f'Dataset Sample {index}\\nTotal Objects: {len(boxes)} | '\n",
    "                f'Class Distribution: {[f\"{i}:{sum(cls==i)}\" for i in range(12)]}',\n",
    "                fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # ë²”ë¡€ ì¶”ê°€\n",
    "    legend_elements = [\n",
    "        patches.Patch(color=colors[i], label=class_names[i]) for i in range(len(colors))\n",
    "    ]\n",
    "    fig.legend(handles=legend_elements, loc='lower center', ncol=6, \n",
    "               bbox_to_anchor=(0.5, -0.05), fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.12)\n",
    "    plt.show()\n",
    "    \n",
    "    # ìƒì„¸ ì •ë³´ ì¶œë ¥\n",
    "    print(f\"ğŸ“Š Sample {index} ìƒì„¸ ì •ë³´:\")\n",
    "    print(f\"  ë©”ì¸ ì´ë¯¸ì§€ í¬ê¸°: {width} x {height}\")\n",
    "    print(f\"  Tissue context í¬ê¸°: {tissue_image.shape[1]} x {tissue_image.shape[0]}\")\n",
    "    print(f\"  ì´ ê°ì²´ ìˆ˜: {len(boxes)}\")\n",
    "    print(f\"  í´ë˜ìŠ¤ë³„ ë¶„í¬:\")\n",
    "    for class_id in range(12):\n",
    "        count = sum(cls == class_id)\n",
    "        if count > 0:\n",
    "            print(f\"    Class {class_id}: {count}ê°œ\")\n",
    "    \n",
    "    print(\"\\nğŸ” ì²˜ìŒ 5ê°œ ê°ì²´ ì •ë³´:\")\n",
    "    for i in range(min(5, len(boxes))):\n",
    "        center_x, center_y, box_w, box_h = boxes[i]\n",
    "        class_id = int(cls[i])\n",
    "        class_name = class_names.get(class_id, f'Class {class_id}')\n",
    "        print(f\"  ê°ì²´ {i+1}: {class_name} | ì¤‘ì‹¬ì : ({center_x:.3f}, {center_y:.3f}) | í¬ê¸°: {box_w:.3f} x {box_h:.3f}\")\n",
    "\n",
    "# train_dataset ì‹œê°í™” ì‹¤í–‰\n",
    "print(\"ğŸ–¼ï¸ Train Dataset ìƒ˜í”Œ ì‹œê°í™” (ë©”ì¸ ì´ë¯¸ì§€ + Tissue Context)\")\n",
    "print(\"=\" * 70)\n",
    "visualize_sample_with_overlay(train_dataset, index=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac886b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_dataset[14][0].numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872981f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ëª¨ë¸ ë° íŒŒë¼ë¯¸í„° ì¤€ë¹„ (Tissue context í™œì„±í™”)\n",
    "model = ContextNn.yolo_v11_m(len(params['names']), use_context=True, context_backbone='efficientnet_b1').to(device)\n",
    "optimizer = torch.optim.AdamW(util.set_params(model, params['weight_decay']),\n",
    "                             lr=2e-4, \n",
    "                             betas=(0.9, 0.999),\n",
    "                             weight_decay=params['weight_decay'])\n",
    "criterion = util.ComputeLoss(model, params)\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë“œ (ì•ˆì „í•œ í•¨ìˆ˜ ì‚¬ìš©)\n",
    "batch_size = 32\n",
    "# ì•ˆì „í•˜ê²Œ ë°ì´í„°ë¡œë” ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "def create_safe_loader(dataset, batch_size, is_train=True):\n",
    "    \"\"\"\n",
    "    ë°°ì¹˜ í¬ê¸°ì— ë§ê²Œ ë°ì´í„°ì…‹ì„ ì¡°ì •í•˜ì—¬ ì•ˆì „í•˜ê²Œ ë°ì´í„°ë¡œë”ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    dataset_size = len(dataset)\n",
    "    \n",
    "    # ë°°ì¹˜ í¬ê¸°ê°€ ë°ì´í„°ì…‹ í¬ê¸°ë³´ë‹¤ í° ê²½ìš° ë°°ì¹˜ í¬ê¸° ì¡°ì •\n",
    "    if dataset_size < batch_size:\n",
    "        print(f\"ê²½ê³ : ë°ì´í„°ì…‹ í¬ê¸°({dataset_size})ê°€ ë°°ì¹˜ í¬ê¸°({batch_size})ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ {dataset_size}ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.\")\n",
    "        actual_batch_size = max(1, dataset_size)\n",
    "    else:\n",
    "        actual_batch_size = batch_size\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ì´ ë°°ì¹˜ í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ëŠ”ì§€ í™•ì¸\n",
    "    if dataset_size % actual_batch_size != 0:\n",
    "        print(f\"ì°¸ê³ : ë°ì´í„°ì…‹ í¬ê¸°({dataset_size})ê°€ ë°°ì¹˜ í¬ê¸°({actual_batch_size})ë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "        print(f\"ë§ˆì§€ë§‰ ë°°ì¹˜ëŠ” {dataset_size % actual_batch_size}ê°œì˜ ìƒ˜í”Œì„ í¬í•¨í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ë°ì´í„°ë¡œë” ìƒì„±\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=actual_batch_size, \n",
    "        shuffle=is_train,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn1,\n",
    "        drop_last=(not is_train)  # í›ˆë ¨ ì‹œì—ëŠ” ë§ˆì§€ë§‰ ë°°ì¹˜ ìœ ì§€, ê²€ì¦ ì‹œì—ëŠ” ë§ˆì§€ë§‰ ë°°ì¹˜ ì œì™¸\n",
    "    )\n",
    "    \n",
    "    return loader, actual_batch_size\n",
    "# ì•ˆì „í•˜ê²Œ ë°ì´í„°ë¡œë” ìƒì„±\n",
    "loader, train_batch_size = create_safe_loader(train_dataset, batch_size, is_train=True)\n",
    "val_loader, val_batch_size = create_safe_loader(val_dataset, batch_size, is_train=False)\n",
    "\n",
    "print(f\"ìµœì¢… í›ˆë ¨ ë°°ì¹˜ í¬ê¸°: {train_batch_size}\")\n",
    "print(f\"ìµœì¢… ê²€ì¦ ë°°ì¹˜ í¬ê¸°: {val_batch_size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3682b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.valid import compute_point_label_metrics  # Point-label ìµœì í™” ë©”íŠ¸ë¦­\n",
    "from utils.valid import visualize_ground_truth_and_prediction_separately\n",
    "from utils.valid import plot_training_progress\n",
    "\n",
    "\n",
    "# main.pyì˜ train í•¨ìˆ˜ë¥¼ ì°¸ì¡°í•œ ê°œì„ ëœ í•™ìŠµ ë£¨í”„\n",
    "train_losses = []\n",
    "\n",
    "# Point-label ë©”íŠ¸ë¦­ìš© ë¦¬ìŠ¤íŠ¸\n",
    "val_det_recalls = []  # Detection Recall\n",
    "val_cls_accs = []     # Classification Accuracy\n",
    "val_macro_f1s = []    # Macro F1-score\n",
    "val_macro_precisions = []  # Macro Precision\n",
    "val_macro_recalls = []  # Macro Recall\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ í†µê³„ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸ (ê° ì—í­ë§ˆë‹¤ class_stats dict ì €ì¥)\n",
    "val_class_stats_history = []\n",
    "\n",
    "epochs = 10000\n",
    "# scale_list = [256, 288, 320, 352, 384, 416, 448, 480, 512]\n",
    "scale_list = [512]\n",
    "# ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "save_dir = '../../model/Breast_ST_context/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "#ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° \n",
    "checkpoint_path = os.path.join(save_dir, 'last_model.pt')\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device,weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "# main.py ìŠ¤íƒ€ì¼ì˜ ì„¤ì •ë“¤\n",
    "best_macro_Overall_Recall = 0  # ë² ìŠ¤íŠ¸ ëª¨ë¸ ê¸°ì¤€: macro_f1\n",
    "accumulate = max(round(64 / batch_size), 1)  # gradient accumulation steps\n",
    "amp_scale = torch.amp.GradScaler()  # mixed precision scaler\n",
    "\n",
    "print(f\"Gradient accumulation steps: {accumulate}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # í›ˆë ¨\n",
    "    model.train()\n",
    "    \n",
    "    # main.py ìŠ¤íƒ€ì¼ì˜ í‰ê·  ì†ì‹¤ ì¶”ì \n",
    "    avg_box_loss = util.AverageMeter()\n",
    "    avg_cls_loss = util.AverageMeter()\n",
    "    avg_dfl_loss = util.AverageMeter()\n",
    "    \n",
    "    train_pbar = tqdm(enumerate(loader), total=len(loader), desc=f'Epoch {epoch+1}/{epochs} Training')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    current_scale = random.choice(scale_list)\n",
    "    train_dataset.set_scale(current_scale)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn1,\n",
    "        drop_last=True\n",
    "    )\n",
    "    for i, (torch_images, tissue_images, targets) in train_pbar:\n",
    "        step = i + len(loader) * epoch\n",
    "        torch_images = torch_images.to(device).float() / 255.\n",
    "        tissue_images = tissue_images.to(device).float() / 255.\n",
    "        \n",
    "        # Forward pass with mixed precision (tissue context í¬í•¨)\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(torch_images, tissue_context=tissue_images)\n",
    "            loss_box, loss_cls, loss_dfl = criterion(outputs, targets)\n",
    "        \n",
    "        # í‰ê·  ì†ì‹¤ ì—…ë°ì´íŠ¸\n",
    "        avg_box_loss.update(loss_box.item(), torch_images.size(0))\n",
    "        avg_cls_loss.update(loss_cls.item(), torch_images.size(0))\n",
    "        avg_dfl_loss.update(loss_dfl.item(), torch_images.size(0))\n",
    "        \n",
    "        # Loss scaling for gradient accumulation\n",
    "        loss_box *= batch_size  # loss scaled by batch_size\n",
    "        loss_cls *= batch_size  # loss scaled by batch_size  \n",
    "        loss_dfl *= batch_size  # loss scaled by batch_size\n",
    "        \n",
    "        total_loss = loss_box + loss_cls + loss_dfl\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        amp_scale.scale(total_loss).backward()\n",
    "        \n",
    "        # Gradient accumulation ë° optimization\n",
    "        if step % accumulate == 0:\n",
    "            # Gradient clipping ë° optimization\n",
    "            amp_scale.step(optimizer)\n",
    "            amp_scale.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # GPU ë©”ëª¨ë¦¬ ë™ê¸°í™”\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # ì§„í–‰ë¥  í‘œì‹œ ì—…ë°ì´íŠ¸ (main.py ìŠ¤íƒ€ì¼)\n",
    "        memory = f'{torch.cuda.memory_reserved() / 1E9:.4g}G'\n",
    "        s = f'Memory: {memory} | Box: {avg_box_loss.avg:.3f} | Cls: {avg_cls_loss.avg:.3f} | DFL: {avg_dfl_loss.avg:.3f}'\n",
    "        train_pbar.set_description(f'Epoch {epoch+1}/{epochs} | {s}')\n",
    "    \n",
    "    # ì—í­ í‰ê·  ì†ì‹¤ ê³„ì‚°\n",
    "    avg_train_loss = avg_box_loss.avg + avg_cls_loss.avg + avg_dfl_loss.avg\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_dataset.set_scale(512)\n",
    "    \n",
    "    # Point-label ìµœì í™” ë©”íŠ¸ë¦­ ê³„ì‚° (ì£¼ìš” í‰ê°€ ì§€í‘œ)\n",
    "    point_metrics = compute_point_label_metrics(\n",
    "        model, val_loader, device, params, distance_threshold=16\n",
    "    )\n",
    "    \n",
    "    detection_recall = point_metrics.get('detection_recall', 0)\n",
    "    cls_accuracy = point_metrics.get('classification_accuracy', 0)\n",
    "    macro_precision = point_metrics.get('macro_precision', 0)\n",
    "    macro_recall = point_metrics.get('macro_recall', 0)\n",
    "    macro_f1 = point_metrics.get('macro_f1', 0)\n",
    "    overall_recall = point_metrics.get('overall_recall', 0)\n",
    "    class_stats = point_metrics.get('class_stats', {})\n",
    "    \n",
    "    val_det_recalls.append(detection_recall)\n",
    "    val_cls_accs.append(cls_accuracy)\n",
    "    val_macro_f1s.append(macro_f1)\n",
    "    val_macro_precisions.append(macro_precision)\n",
    "    val_macro_recalls.append(macro_recall)\n",
    "    \n",
    "    # í´ë˜ìŠ¤ë³„ í†µê³„ ì €ì¥\n",
    "    val_class_stats_history.append(class_stats)\n",
    "    \n",
    "    # ê²°ê³¼ ì¶œë ¥\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs} Results:\")\n",
    "    print(f\"  Train Loss - Box: {avg_box_loss.avg:.4f}, Cls: {avg_cls_loss.avg:.4f}, DFL: {avg_dfl_loss.avg:.4f}, Total: {avg_train_loss:.4f}\")\n",
    "    print(f\"ğŸ¯ Point-Label Metrics:\")\n",
    "    print(f\"  Detection Recall: {detection_recall:.4f} (GT ì¤‘ ì°¾ì€ ë¹„ìœ¨)\")\n",
    "    print(f\"  Classification Accuracy: {cls_accuracy:.4f} (ì°¾ì€ ì„¸í¬ì˜ ë¶„ë¥˜ ì •í™•ë„)\")\n",
    "    print(f\"  Macro Precision: {macro_precision:.4f}\")\n",
    "    print(f\"  Macro Recall: {macro_recall:.4f}\")\n",
    "    print(f\"  Macro F1-score: {macro_f1:.4f} â­\")\n",
    "    print(f\"  Overall Recall: {overall_recall:.4f}\")\n",
    "    \n",
    "    \n",
    "    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥ (Macro F1 ê¸°ì¤€)\n",
    "    if overall_recall > best_macro_Overall_Recall:\n",
    "        best_macro_Overall_Recall = overall_recall\n",
    "        save_checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'amp_scale_state_dict': amp_scale.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'box_loss': avg_box_loss.avg,\n",
    "            'cls_loss': avg_cls_loss.avg,\n",
    "            'dfl_loss': avg_dfl_loss.avg,\n",
    "            # Point-label ë©”íŠ¸ë¦­\n",
    "            'detection_recall': detection_recall,\n",
    "            'classification_accuracy': cls_accuracy,\n",
    "            'macro_precision': macro_precision,\n",
    "            'macro_recall': macro_recall,\n",
    "            'macro_f1': macro_f1,\n",
    "            'overall_recall': overall_recall,\n",
    "            'class_stats': class_stats,\n",
    "            'params': params\n",
    "        }\n",
    "        torch.save(save_checkpoint, os.path.join(save_dir, 'best_model.pt'))\n",
    "        print(f\"ğŸ‰ ìƒˆë¡œìš´ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥! overall recall: {overall_recall:.4f}\")\n",
    "    \n",
    "    # ìµœì‹  ëª¨ë¸ë„ ì €ì¥ (main.py ìŠ¤íƒ€ì¼)\n",
    "    last_checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'amp_scale_state_dict': amp_scale.state_dict(),\n",
    "        'train_loss': avg_train_loss,\n",
    "        'box_loss': avg_box_loss.avg,\n",
    "        'cls_loss': avg_cls_loss.avg,\n",
    "        'dfl_loss': avg_dfl_loss.avg,\n",
    "        # Point-label ë©”íŠ¸ë¦­\n",
    "        'detection_recall': detection_recall,\n",
    "        'classification_accuracy': cls_accuracy,\n",
    "        'macro_precision': macro_precision,\n",
    "        'macro_recall': macro_recall,\n",
    "        'macro_f1': macro_f1,\n",
    "        'overall_recall': overall_recall,\n",
    "        'class_stats': class_stats,\n",
    "        'params': params\n",
    "    }\n",
    "    torch.save(last_checkpoint, os.path.join(save_dir, 'last_model.pt'))\n",
    "    \n",
    "    # 100 ì—í­ë§ˆë‹¤ í•™ìŠµ ì§„í–‰ ê·¸ë˜í”„ ìƒì„± ë° ì €ì¥ (Point-label ë©”íŠ¸ë¦­ + í´ë˜ìŠ¤ë³„ ì„±ëŠ¥)\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        try:\n",
    "            print(f\"\\nğŸ“Š Epoch {epoch+1} - í•™ìŠµ ì§„í–‰ ìƒí™© ê·¸ë˜í”„ ìƒì„± ì¤‘...\")\n",
    "            plot_training_progress(\n",
    "                train_losses, val_det_recalls, val_cls_accs, val_macro_precisions, \n",
    "                val_macro_recalls, val_macro_f1s, epoch+1, save_dir, \n",
    "                class_stats_history=val_class_stats_history\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"ê·¸ë˜í”„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    # ê°œì„ ëœ ê²€ì¦ ì´ë¯¸ì§€ ì‹œê°í™” (ë§¤ 10 ì—í­ë§ˆë‹¤) - ì‹¤ì œ ë¼ë²¨ê³¼ ì˜ˆì¸¡ ë¼ë²¨ì„ ë³„ë„ figureë¡œ í‘œì‹œ\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        try:\n",
    "            # ì—¬ëŸ¬ ìƒ˜í”Œì— ëŒ€í•´ ì‹œê°í™”\n",
    "            num_samples = 1 # ìƒ˜í”Œ ìˆ˜ë¥¼ 1ê°œ\n",
    "            for sample_idx in range(num_samples):\n",
    "                print(f\"\\nğŸ“Š Epoch {epoch+1} - ê²€ì¦ ìƒ˜í”Œ {sample_idx+1}/{num_samples}:\")\n",
    "                print(\"=\" * 60)\n",
    "                \n",
    "                # ì‹¤ì œ ë¼ë²¨ê³¼ ì˜ˆì¸¡ ë¼ë²¨ì„ ë³„ë„ figureë¡œ í‘œì‹œ\n",
    "                sample_idx = random.randint(0, len(val_dataset)-1)\n",
    "                visualize_ground_truth_and_prediction_separately(\n",
    "                    model, val_dataset, idx=sample_idx, \n",
    "                    epoch=epoch+1, save_dir=save_dir\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ì‹œê°í™” ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "print(\"ğŸ¯ í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(f\"ìµœì¢… ë² ìŠ¤íŠ¸ Overall Recall: {best_macro_Overall_Recall:.4f}\")\n",
    "print(f\"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {save_dir}\")\n",
    "print(f\"ë² ìŠ¤íŠ¸ ëª¨ë¸: {os.path.join(save_dir, 'best_model.pt')}\")\n",
    "print(f\"ìµœì‹  ëª¨ë¸: {os.path.join(save_dir, 'last_model.pt')}\")\n",
    "\n",
    "# ìµœì¢… ì„±ëŠ¥ ìš”ì•½\n",
    "if val_macro_f1s:\n",
    "    final_macro_f1 = val_macro_f1s[-1]\n",
    "    final_det_recall = val_det_recalls[-1]\n",
    "    final_cls_acc = val_cls_accs[-1]\n",
    "    final_macro_precision = val_macro_precisions[-1]\n",
    "    final_macro_recall = val_macro_recalls[-1]\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ìµœì¢… ì„±ëŠ¥ ìš”ì•½ (Point-Label Metrics):\")\n",
    "    print(f\"  Macro F1-score: {final_macro_f1:.4f} â­\")\n",
    "    print(f\"  Macro Precision: {final_macro_precision:.4f}\")\n",
    "    print(f\"  Macro Recall: {final_macro_recall:.4f}\")\n",
    "    print(f\"  Detection Recall: {final_det_recall:.4f}\")\n",
    "    print(f\"  Classification Accuracy: {final_cls_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf945402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.valid import compute_point_label_metrics  # Point-label ìµœì í™” ë©”íŠ¸ë¦­\n",
    "from utils.valid import visualize_ground_truth_and_prediction_separately\n",
    "from utils.valid import plot_training_progress\n",
    "\n",
    "\n",
    "# main.pyì˜ train í•¨ìˆ˜ë¥¼ ì°¸ì¡°í•œ ê°œì„ ëœ í•™ìŠµ ë£¨í”„\n",
    "train_losses = []\n",
    "\n",
    "# Point-label ë©”íŠ¸ë¦­ìš© ë¦¬ìŠ¤íŠ¸\n",
    "val_det_recalls = []  # Detection Recall\n",
    "val_cls_accs = []     # Classification Accuracy\n",
    "val_macro_f1s = []    # Macro F1-score\n",
    "val_macro_precisions = []  # Macro Precision\n",
    "val_macro_recalls = []  # Macro Recall\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ í†µê³„ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸ (ê° ì—í­ë§ˆë‹¤ class_stats dict ì €ì¥)\n",
    "val_class_stats_history = []\n",
    "\n",
    "epochs = 10000\n",
    "# scale_list = [256, 288, 320, 352, 384, 416, 448, 480, 512]\n",
    "scale_list = [512]\n",
    "# ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "save_dir = '../../model/Breast_ST_context/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "#ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° \n",
    "checkpoint_path = os.path.join(save_dir, 'last_model.pt')\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device,weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "sample_idx = random.randint(0, len(val_dataset)-1)\n",
    "visualize_ground_truth_and_prediction_separately(\n",
    "                    model, val_dataset, idx=sample_idx, \n",
    "                    epoch=1, save_dir=save_dir,conf_threshold=0.1,\n",
    "                    iou_threshold=0.1\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbce59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohen's Kappa ê´€ë ¨ í•¨ìˆ˜ë“¤ì„ valid.pyì—ì„œ import\n",
    "from utils.valid import compute_validation_metrics_with_kappa, get_kappa_interpretation, quick_kappa_test\n",
    "\n",
    "# í˜„ì¬ ëª¨ë¸ì˜ ë¹ ë¥¸ Cohen's Kappa í…ŒìŠ¤íŠ¸\n",
    "print(\"ğŸ” í˜„ì¬ ëª¨ë¸ì˜ Cohen's Kappa ë¹ ë¥¸ ì¸¡ì •...\")\n",
    "current_kappa = quick_kappa_test(model, val_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
